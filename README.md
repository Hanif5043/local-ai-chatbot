\# Local AI Knowledge Chatbot



Runs completely on a local machine using Ollama and LangChain. No cloud key.



This app lets a user

1\. Upload PDF or text documents

2\. Build a local vector store using sentence transformer embeddings

3\. Ask questions about the uploaded content

4\. Get answers from a local LLM served by Ollama



The goal is to show a simple but complete Retrieval Augmented Generation pipeline that anyone can run on Windows.



\## Features



\- Local model through Ollama (tested with `llama3`)

\- Document loader for PDF and TXT

\- Text splitting using `langchain-text-splitters`

\- Vector store using Chroma with persistence

\- Streamlit user interface

\- Works in a virtual environment

\- Good starter project for ML or data science portfolios



\## Prerequisites



1\. Windows 10 or 11

2\. \[Ollama](https://ollama.com/) installed and running

Â   Ollama must be available at `http://localhost:11434`

3\. Python 3.12 or newer

4\. Git



\## Setup



```bash

git clone https://github.com/<your-username>/local-ai-chatbot.git

cd local-ai-chatbot

python -m venv venv

venv\\\\Scripts\\\\activate

pip install -r requirements.txt







Run

streamlit run app.py





Then open the URL shown in the terminal, usually

http://localhost:8501



How to use



Upload one or more PDF or TXT files in the sidebar



Click "Build knowledge base"



Type a question related to the files



The answer is generated by the local LLM







## Demo



Here are some screenshots of the working application:



\### ðŸ§  Building Knowledge Base

!\[Build Base](./assets/vector.png)



\### ðŸ’¬ Asking Questions

!\[Ask Question](./assets/output.png)









Notes



The first run will download the embedding model all-MiniLM-L6-v2



Chroma will create a storage/ directory for the vector database



If you change the documents, delete storage/ and rebuild














